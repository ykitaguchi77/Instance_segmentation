{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+mljuhiAVaodlXY8s2qK2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Instance_segmentation/blob/main/MPLD_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MPLD analysis**"
      ],
      "metadata": {
        "id": "h1DBeaeIsrIF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQwGfev4S5bq",
        "outputId": "793085b0-e98d-4cd0-8625-d164623331be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: gdrive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detect_model_path = \"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/models/295+cerebhq1-20000_yolo11l.pt\"\n",
        "seg_model_path = \"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/models/eyelid_caruncle_yolo11seg_1-139.pt\"\n",
        "obb_model_path = \"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/models/yolo11n_obb_1-295_1to139.pt\""
      ],
      "metadata": {
        "id": "7RJFGmsJTiLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/Segmentation_test_images/Control_adult/1000.jpg\""
      ],
      "metadata": {
        "id": "034O76ThU0IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics --q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "69ApZnGEbnFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from PIL import Image, ImageDraw\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Driveをマウント (Colabで実行する場合)\n",
        "# try:\n",
        "#     drive.mount('/content/drive')\n",
        "# except OSError as e:\n",
        "#     if \"already mounted\" in str(e).lower():\n",
        "#         print(\"Google Drive is already mounted.\")\n",
        "#     else:\n",
        "#         raise e\n",
        "\n",
        "# --- 1. パス設定 ---\n",
        "detect_model_path = \"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/models/295+cerebhq1-20000_yolo11l.pt\"\n",
        "# seg_model_path と obb_model_path はこのスクリプトでは使用しません\n",
        "# seg_model_path = \"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/models/eyelid_caruncle_yolo11seg_1-139.pt\"\n",
        "# obb_model_path = \"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/models/yolo11n_obb_1-295_1to139.pt\"\n",
        "image_path = \"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/Segmentation_test_images/Control_adult/1000.jpg\"\n",
        "\n",
        "# --- 2. モデルのロード ---\n",
        "try:\n",
        "    detect_model = YOLO(detect_model_path)\n",
        "    print(f\"Detection model '{detect_model_path}' loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading detection model: {e}\")\n",
        "    # スクリプトの実行をここで停止するか、適切に処理\n",
        "    exit()\n",
        "\n",
        "# --- 3. 画像のロード ---\n",
        "try:\n",
        "    img_pil = Image.open(image_path).convert(\"RGB\")\n",
        "    img_cv_rgb = np.array(img_pil) # OpenCVで処理するためにNumpy配列 (RGB) に変換\n",
        "    original_img_height, original_img_width = img_cv_rgb.shape[:2]\n",
        "    print(f\"Image '{image_path}' loaded successfully. Dimensions: {original_img_width}x{original_img_height}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Image file not found at '{image_path}'\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading image: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 4. 物体検出の実行 ---\n",
        "print(\"Running detection model...\")\n",
        "results = detect_model(img_pil, verbose=False) # verbose=Falseで検出ログを抑制\n",
        "print(\"Detection complete.\")\n",
        "\n",
        "# --- 5. BBoxの処理と画像の切り抜き ---\n",
        "cropped_images_pil = []\n",
        "\n",
        "if results and len(results) > 0 and results[0].boxes:\n",
        "    boxes = results[0].boxes.xyxy.cpu().numpy()  # バウンディングボックスをxyxy形式で取得\n",
        "    print(f\"Found {len(boxes)} bounding box(es).\")\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "        bbox_width = x2 - x1\n",
        "        bbox_height = y2 - y1\n",
        "        bbox_center_x = x1 + bbox_width / 2.0\n",
        "        bbox_center_y = y1 + bbox_height / 2.0\n",
        "\n",
        "        print(f\"\\nProcessing BBox {i+1}: [x1={x1}, y1={y1}, x2={x2}, y2={y2}]\")\n",
        "        print(f\"  Original BBox width: {bbox_width}, height: {bbox_height}, center: ({bbox_center_x:.2f}, {bbox_center_y:.2f})\")\n",
        "\n",
        "        # 横幅を25%ずつ水増し (元の1.5倍)\n",
        "        new_width = int(round(bbox_width * 1.5))\n",
        "        # 縦幅は新しい横幅と一致させる\n",
        "        new_height = new_width\n",
        "\n",
        "        print(f\"  Target new width: {new_width}, new height: {new_height}\")\n",
        "\n",
        "        # 切り抜き用の座標を計算 (中心を維持)\n",
        "        crop_x1 = int(round(bbox_center_x - new_width / 2.0))\n",
        "        crop_y1 = int(round(bbox_center_y - new_height / 2.0))\n",
        "        crop_x2 = crop_x1 + new_width\n",
        "        crop_y2 = crop_y1 + new_height\n",
        "\n",
        "        print(f\"  Calculated crop window: [x1'={crop_x1}, y1'={crop_y1}, x2'={crop_x2}, y2'={crop_y2}]\")\n",
        "\n",
        "        # パディング量の計算\n",
        "        pad_left = max(0, -crop_x1)\n",
        "        pad_top = max(0, -crop_y1)\n",
        "        pad_right = max(0, crop_x2 - original_img_width)\n",
        "        pad_bottom = max(0, crop_y2 - original_img_height)\n",
        "\n",
        "        print(f\"  Padding needed: top={pad_top}, bottom={pad_bottom}, left={pad_left}, right={pad_right}\")\n",
        "\n",
        "        # 実際に画像から切り取る範囲を調整 (画像範囲内に収める)\n",
        "        actual_crop_x1_img = crop_x1 + pad_left\n",
        "        actual_crop_y1_img = crop_y1 + pad_top\n",
        "        actual_crop_x2_img = crop_x2 - pad_right\n",
        "        actual_crop_y2_img = crop_y2 - pad_bottom\n",
        "\n",
        "        print(f\"  Actual crop from image: [x_start={actual_crop_x1_img}, y_start={actual_crop_y1_img}, x_end={actual_crop_x2_img}, y_end={actual_crop_y2_img}]\")\n",
        "\n",
        "        # 画像を切り抜く (Numpy配列スライス)\n",
        "        # スライスする前に、座標が画像の範囲内にあることを確認\n",
        "        actual_crop_x1_img = max(0, actual_crop_x1_img)\n",
        "        actual_crop_y1_img = max(0, actual_crop_y1_img)\n",
        "        actual_crop_x2_img = min(original_img_width, actual_crop_x2_img)\n",
        "        actual_crop_y2_img = min(original_img_height, actual_crop_y2_img)\n",
        "\n",
        "        if actual_crop_x1_img >= actual_crop_x2_img or actual_crop_y1_img >= actual_crop_y2_img:\n",
        "            print(f\"  Skipping BBox {i+1} due to invalid crop dimensions after clamping.\")\n",
        "            # 有効な切り抜き領域がない場合は、黒一色の画像を生成するか、スキップ\n",
        "            # ここでは、指定サイズの黒画像を生成\n",
        "            cropped_part = np.zeros((new_height, new_width, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            cropped_part_from_img = img_cv_rgb[actual_crop_y1_img:actual_crop_y2_img, actual_crop_x1_img:actual_crop_x2_img]\n",
        "            # パディングを適用\n",
        "            # cv2.copyMakeBorderのvalueはBGRだが、黒(0,0,0)なのでRGBでも同じ\n",
        "            cropped_part = cv2.copyMakeBorder(\n",
        "                cropped_part_from_img,\n",
        "                pad_top,\n",
        "                pad_bottom,\n",
        "                pad_left,\n",
        "                pad_right,\n",
        "                cv2.BORDER_CONSTANT,\n",
        "                value=[0, 0, 0]  # 黒でパディング\n",
        "            )\n",
        "\n",
        "        # 稀にパディング計算や丸め誤差で1pxずれることがある場合、リサイズで最終サイズを保証\n",
        "        if cropped_part.shape[0] != new_height or cropped_part.shape[1] != new_width:\n",
        "            print(f\"  Warning: Padded crop size ({cropped_part.shape[1]}x{cropped_part.shape[0]}) differs from target ({new_width}x{new_height}). Resizing.\")\n",
        "            cropped_part = cv2.resize(cropped_part, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        cropped_images_pil.append(Image.fromarray(cropped_part)) # Pillow Imageとしてリストに追加\n",
        "        print(f\"  Cropped and padded image {i+1} generated. Final size: {cropped_part.shape[1]}x{cropped_part.shape[0]}\")\n",
        "\n",
        "else:\n",
        "    print(\"No objects detected or no bounding boxes found in the results.\")\n",
        "\n",
        "# --- 6. 切り抜いた画像の表示 ---\n",
        "if cropped_images_pil:\n",
        "    num_cropped = len(cropped_images_pil)\n",
        "    # 表示する画像の数に応じて subplot のレイアウトを調整\n",
        "    cols = min(num_cropped, 3) # 最大3列で表示\n",
        "    rows = (num_cropped + cols - 1) // cols # 必要な行数\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n",
        "    axes = np.array(axes).ravel() # axesを1次元配列に変換して扱いやすくする\n",
        "\n",
        "    for i, cropped_img_pil in enumerate(cropped_images_pil):\n",
        "        axes[i].imshow(cropped_img_pil)\n",
        "        axes[i].set_title(f\"Cropped Image {i+1}\")\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "    # 余分な subplot を非表示にする\n",
        "    for j in range(num_cropped, len(axes)):\n",
        "        axes[j].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"\\nDisplayed {num_cropped} cropped image(s).\")\n",
        "elif not (results and len(results) > 0 and results[0].boxes):\n",
        "    pass # すでに \"No objects detected...\" のメッセージが表示されている\n",
        "else:\n",
        "    print(\"No valid bounding boxes were processed to generate cropped images.\")\n",
        "\n",
        "\n",
        "# --- 7. 追加モデルのロード ---\n",
        "seg_model_path = \"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/models/eyelid_caruncle_yolo11seg_1-139.pt\"\n",
        "obb_model_path = \"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/models/yolo11n_obb_1-295_1to139.pt\"\n",
        "\n",
        "print(\"\\n--- Loading additional models ---\")\n",
        "try:\n",
        "    seg_model = YOLO(seg_model_path)\n",
        "    print(f\"Segmentation model '{seg_model_path}' loaded successfully.\")\n",
        "    obb_model = YOLO(obb_model_path)\n",
        "    print(f\"OBB model '{obb_model_path}' loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading additional models: {e}\")\n",
        "    # エラーが発生した場合、以降の処理をスキップするかどうかを決定\n",
        "    # ここでは、エラーメッセージを表示して終了する代わりに、merged_images_pilが空のまま処理を進める\n",
        "    seg_model = None\n",
        "    obb_model = None\n",
        "\n",
        "\n",
        "# --- 8. 各切り抜き画像への推論と結果のマージ ---\n",
        "merged_images_pil = []\n",
        "\n",
        "if not cropped_images_pil:\n",
        "    print(\"No cropped images from the previous step to process.\")\n",
        "elif seg_model is None or obb_model is None:\n",
        "    print(\"One or more additional models failed to load. Skipping merge step.\")\n",
        "else:\n",
        "    print(f\"\\n--- Processing {len(cropped_images_pil)} cropped image(s) with segmentation and OBB models ---\")\n",
        "    for i, base_cropped_pil in enumerate(cropped_images_pil):\n",
        "        print(f\"  Processing cropped image {i+1}/{len(cropped_images_pil)}...\")\n",
        "\n",
        "        # 元の切り抜き画像 (Pillow RGB) をコピーして作業用にする\n",
        "        # 推論は元のクリーンな画像で行う\n",
        "        current_pil_image = base_cropped_pil.copy()\n",
        "        current_rgb_numpy = np.array(current_pil_image) # RGB NumPy array\n",
        "\n",
        "        # 1. セグメンテーション推論と描画\n",
        "        # plot()はBGRのNumPy配列を返す。入力imgはRGB/BGRのNumPy配列を受け付ける。\n",
        "        # ここでは、まずセグメンテーションの結果を元のRGB画像に描画する。\n",
        "        # plot()のimgに渡す配列はコピーする方が安全。\n",
        "        img_with_seg_bgr = None\n",
        "        try:\n",
        "            print(f\"    Running segmentation model on image {i+1} with retina_masks=True...\")\n",
        "            results_seg = seg_model(\n",
        "                current_pil_image,\n",
        "                verbose=False,\n",
        "                conf=0.35, # confはお好みで調整\n",
        "                retina_masks=True # ★ 高品質マスク生成オプションを追加\n",
        "            )\n",
        "            if results_seg and results_seg[0].masks is not None: # マスクが存在する場合のみ描画\n",
        "                img_with_seg_bgr = results_seg[0].plot(\n",
        "                    img=current_rgb_numpy.copy(), # 元のRGB画像に描画\n",
        "                    pil=False,                    # NumPy配列(BGR)で結果取得\n",
        "                    masks=True,                   # マスクを描画\n",
        "                    boxes=False,                  # セグメンテーションのBBoxは描画しない (任意)\n",
        "                    labels=True                   # ラベルを描画\n",
        "                )\n",
        "                print(f\"    Segmentation applied to image {i+1}.\")\n",
        "            else:\n",
        "                print(f\"    No segmentation masks found by seg_model for image {i+1}.\")\n",
        "                # マスクがない場合は、元の画像をBGRに変換したものを使用\n",
        "                img_with_seg_bgr = cv2.cvtColor(current_rgb_numpy.copy(), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error during segmentation on image {i+1}: {e}\")\n",
        "            # エラー発生時は、元の画像をBGRに変換したものを次のステップのベースとする\n",
        "            img_with_seg_bgr = cv2.cvtColor(current_rgb_numpy.copy(), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "\n",
        "        # 2. OBB推論と描画\n",
        "        # OBBモデルの推論は、元のクリーンな画像 (current_pil_image) で行う。\n",
        "        # 描画は、セグメンテーションが描画された画像 (img_with_seg_bgr) に対して行う。\n",
        "        final_img_np_bgr = None\n",
        "        try:\n",
        "            print(f\"    Running OBB model on image {i+1}...\")\n",
        "            results_obb = obb_model(current_pil_image, verbose=False, conf=0.35) # confはお好みで調整\n",
        "            if results_obb and hasattr(results_obb[0], 'obb') and results_obb[0].obb is not None: # OBBが存在する場合のみ描画\n",
        "                 # OBBの結果を、セグメンテーションが描画済みの画像 (img_with_seg_bgr) に重ねて描画\n",
        "                final_img_np_bgr = results_obb[0].plot(\n",
        "                    img=img_with_seg_bgr.copy(), # セグメンテーション描画済みのBGR画像\n",
        "                    pil=False,                 # NumPy配列(BGR)で結果取得\n",
        "                    masks=False,               # OBBモデルは通常マスク出力しない\n",
        "                    boxes=True,                # OBBを描画 (これがOBBを描画するはず)\n",
        "                    labels=True                # ラベルを描画\n",
        "                )\n",
        "                print(f\"    OBB applied to image {i+1}.\")\n",
        "            else:\n",
        "                print(f\"    No OBB detections found by obb_model for image {i+1}.\")\n",
        "                final_img_np_bgr = img_with_seg_bgr.copy() # OBBがない場合は直前の画像\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error during OBB detection on image {i+1}: {e}\")\n",
        "            # エラー発生時は、セグメンテーション描画後の画像を最終結果とする\n",
        "            final_img_np_bgr = img_with_seg_bgr.copy()\n",
        "\n",
        "        # 最終結果 (BGR NumPy array) をPillow Image (RGB) に変換してリストに追加\n",
        "        final_img_pil = Image.fromarray(cv2.cvtColor(final_img_np_bgr, cv2.COLOR_BGR2RGB))\n",
        "        merged_images_pil.append(final_img_pil)\n",
        "        print(f\"  Finished processing and merging for image {i+1}.\")\n",
        "\n",
        "# --- 9. マージされた画像の表示 ---\n",
        "if merged_images_pil:\n",
        "    print(f\"\\n--- Displaying {len(merged_images_pil)} merged image(s) ---\")\n",
        "    num_merged = len(merged_images_pil)\n",
        "    # 表示する画像の数に応じて subplot のレイアウトを調整\n",
        "    cols = min(num_merged, 3) # 最大3列で表示\n",
        "    rows = (num_merged + cols - 1) // cols # 必要な行数\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(7 * cols, 7 * rows)) # figsizeを少し調整\n",
        "    if num_merged == 1: # 画像が1枚の場合、axesはオブジェクトであり、配列ではない\n",
        "        axes = np.array([axes])\n",
        "    axes = axes.ravel() # axesを1次元配列に変換して扱いやすくする\n",
        "\n",
        "    for i, merged_img_pil in enumerate(merged_images_pil):\n",
        "        axes[i].imshow(merged_img_pil)\n",
        "        axes[i].set_title(f\"Merged Result {i+1}\")\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "    # 余分な subplot があれば非表示にする\n",
        "    for j in range(num_merged, len(axes)):\n",
        "        axes[j].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    if cropped_images_pil and (seg_model is not None and obb_model is not None):\n",
        "        print(\"No images were successfully merged. Check logs for errors.\")\n",
        "    # cropped_images_pilが空だった場合やモデルロード失敗のメッセージは既に表示されている"
      ],
      "metadata": {
        "id": "1j_hGq48a5Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C0Jah8V-pvdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install japanize-matplotlib --q\n",
        "import japanize_matplotlib # インポートするだけで日本語対応が改善されます"
      ],
      "metadata": {
        "id": "Lep86SEfpve4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# --- 定数定義 ---\n",
        "OBB_CLASS_PUPIL = 'Pupil'\n",
        "OBB_CLASS_IRIS = 'Iris'\n",
        "SEG_CLASS_EYELID = 'Eyelid'\n",
        "CORNEAL_DIAMETER_MM = 12.0\n",
        "RADIAL_ANGLES_DEG = [0, 15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180]\n",
        "\n",
        "# --- 出力ディレクトリ作成 ---\n",
        "output_dir = \"/content/mrmpd_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"結果は '{output_dir}' ディレクトリに保存されます。\")\n",
        "\n",
        "# --- ヘルパー関数: OBBデータ抽出 (変更なし) ---\n",
        "def get_obb_data_by_class_name(obb_results_obj, target_class_name, model_obb_names, img_idx_for_debug=\"N/A\"):\n",
        "    if obb_results_obj is None or not hasattr(obb_results_obj, 'obb') or obb_results_obj.obb is None or \\\n",
        "       not hasattr(obb_results_obj.obb, 'cls') or len(obb_results_obj.obb.cls) == 0:\n",
        "        return None\n",
        "    target_cls_indices = []\n",
        "    if isinstance(model_obb_names, dict):\n",
        "        for cls_id, name in model_obb_names.items():\n",
        "            if name == target_class_name:\n",
        "                target_cls_indices.append(int(cls_id))\n",
        "    elif isinstance(model_obb_names, list):\n",
        "        for cls_id, name in enumerate(model_obb_names):\n",
        "            if name == target_class_name:\n",
        "                target_cls_indices.append(cls_id)\n",
        "    if not target_cls_indices: return None\n",
        "    best_obb_data = None\n",
        "    highest_conf = -1.0\n",
        "    if not hasattr(obb_results_obj.obb, 'conf') or not hasattr(obb_results_obj.obb, 'xywhr'):\n",
        "        return None\n",
        "    detected_classes = obb_results_obj.obb.cls.cpu().numpy().astype(int)\n",
        "    confidences = obb_results_obj.obb.conf.cpu().numpy()\n",
        "    xywhr_data = obb_results_obj.obb.xywhr.cpu().numpy()\n",
        "    found_instances = 0\n",
        "    for i in range(len(detected_classes)):\n",
        "        if detected_classes[i] in target_cls_indices:\n",
        "            found_instances +=1\n",
        "            current_conf = confidences[i]\n",
        "            if current_conf > highest_conf:\n",
        "                highest_conf = current_conf\n",
        "                best_obb_data = {\n",
        "                    \"center_x\": xywhr_data[i][0], \"center_y\": xywhr_data[i][1],\n",
        "                    \"width\": xywhr_data[i][2], \"height\": xywhr_data[i][3],\n",
        "                    \"angle_rad\": xywhr_data[i][4], \"confidence\": current_conf\n",
        "                }\n",
        "    return best_obb_data if found_instances > 0 and best_obb_data is not None else None\n",
        "\n",
        "# --- ヘルパー関数: セグメンテーションマスク抽出 (変更なし) ---\n",
        "def get_mask_by_class_name(seg_results_obj, target_class_name, model_seg_names, target_shape_hw, img_idx_for_debug=\"N/A\"):\n",
        "    if seg_results_obj.masks is None: return None\n",
        "    if not hasattr(seg_results_obj.masks, 'data') or seg_results_obj.masks.data is None: return None\n",
        "    if seg_results_obj.boxes is None: return None\n",
        "    target_cls_indices = []\n",
        "    if isinstance(model_seg_names, dict):\n",
        "        for cls_id, name in model_seg_names.items():\n",
        "            if name == target_class_name: target_cls_indices.append(int(cls_id))\n",
        "    elif isinstance(model_seg_names, list):\n",
        "        for cls_id, name in enumerate(model_seg_names):\n",
        "            if name == target_class_name: target_cls_indices.append(cls_id)\n",
        "    if not target_cls_indices: return None\n",
        "    relevant_masks_data = []\n",
        "    all_detected_cls_indices = seg_results_obj.boxes.cls.cpu().numpy().astype(int)\n",
        "    raw_masks_tensor = seg_results_obj.masks.data\n",
        "    if raw_masks_tensor is None or len(raw_masks_tensor) == 0: return None\n",
        "    if len(all_detected_cls_indices) != raw_masks_tensor.shape[0]:\n",
        "        print(f\"WARNING (Image {img_idx_for_debug}, get_mask): Mismatch boxes ({len(all_detected_cls_indices)}) and masks ({raw_masks_tensor.shape[0]}).\")\n",
        "    num_instances_of_target_class_found = 0\n",
        "    for i in range(len(all_detected_cls_indices)):\n",
        "        if i >= raw_masks_tensor.shape[0]: break\n",
        "        current_model_cls_idx = all_detected_cls_indices[i]\n",
        "        if current_model_cls_idx in target_cls_indices:\n",
        "            num_instances_of_target_class_found += 1\n",
        "            try:\n",
        "                mask_proto = raw_masks_tensor[i]\n",
        "                mask_proto_tensor = mask_proto.unsqueeze(0).unsqueeze(0)\n",
        "                if mask_proto_tensor.shape[2:] == target_shape_hw:\n",
        "                    upsampled_mask_tensor = mask_proto_tensor.squeeze()\n",
        "                else:\n",
        "                    upsampled_mask_tensor = torch.nn.functional.interpolate(\n",
        "                        mask_proto_tensor, size=target_shape_hw, mode='bilinear', align_corners=False\n",
        "                    ).squeeze()\n",
        "                mask_np = upsampled_mask_tensor.cpu().numpy()\n",
        "                mask_np_binary = (mask_np > 0.5).astype(np.uint8) * 255\n",
        "                contours, _ = cv2.findContours(mask_np_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                area = sum(cv2.contourArea(c) for c in contours) if contours else 0\n",
        "                if area > 0: relevant_masks_data.append({'mask': mask_np_binary, 'area': area})\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR (Image {img_idx_for_debug}, get_mask): Exception processing mask for instance {i} of '{target_class_name}': {e}\")\n",
        "                continue\n",
        "    if not relevant_masks_data: return None\n",
        "    largest_mask_data = max(relevant_masks_data, key=lambda x: x['area'])\n",
        "    return largest_mask_data['mask']\n",
        "\n",
        "# --- MRMPD解析メイン処理 ---\n",
        "print(\"\\n--- MRMPD解析を開始します ---\")\n",
        "\n",
        "if 'cropped_images_pil' not in locals() or not cropped_images_pil:\n",
        "    print(\"重大なエラー: `cropped_images_pil` が見つからないか空です。\")\n",
        "    exit(\"エラー: 入力画像リスト `cropped_images_pil` が定義されていません。\")\n",
        "if 'obb_model' not in locals() or not hasattr(obb_model, 'names'):\n",
        "    print(\"重大なエラー: `obb_model` がロードされていないか、`names` 属性がありません。\")\n",
        "    exit(\"エラー: `obb_model` が定義されていません。\")\n",
        "if 'seg_model' not in locals() or not hasattr(seg_model, 'names'):\n",
        "    print(\"重大なエラー: `seg_model` がロードされていないか、`names` 属性がありません。\")\n",
        "    exit(\"エラー: `seg_model` が定義されていません。\")\n",
        "\n",
        "all_mrmpd_results_data = []\n",
        "\n",
        "for img_idx, pil_image_clean_crop in enumerate(cropped_images_pil):\n",
        "    base_filename = getattr(pil_image_clean_crop, 'filename', f\"image_{img_idx + 1}\")\n",
        "    base_filename = os.path.splitext(os.path.basename(base_filename))[0]\n",
        "    debug_img_id_str = f\"{base_filename}_idx{img_idx + 1}\"\n",
        "\n",
        "    print(f\"\\n--- 画像 {debug_img_id_str} のMRMPD解析を開始 ---\")\n",
        "    current_mrmpd_data = {\"image_id\": debug_img_id_str, \"status\": \"処理開始\"}\n",
        "\n",
        "    img_np_rgb = np.array(pil_image_clean_crop.convert(\"RGB\"))\n",
        "    img_h, img_w = img_np_rgb.shape[:2]\n",
        "\n",
        "    pupil_center = None\n",
        "    pixels_per_mm = None\n",
        "\n",
        "    # 1. OBBモデルによる瞳孔と虹彩の情報取得 (変更なし)\n",
        "    try:\n",
        "        obb_results_list = obb_model(pil_image_clean_crop, verbose=False, conf=0.25)\n",
        "        if not obb_results_list: raise ValueError(\"OBBモデルが結果を返しませんでした。\")\n",
        "        obb_result_obj = obb_results_list[0]\n",
        "\n",
        "        pupil_obb_data = get_obb_data_by_class_name(obb_result_obj, OBB_CLASS_PUPIL, obb_model.names, img_idx_for_debug=debug_img_id_str)\n",
        "        if not pupil_obb_data: raise ValueError(f\"'{OBB_CLASS_PUPIL}' OBBが見つかりません。\")\n",
        "        pupil_center = (int(round(pupil_obb_data[\"center_x\"])), int(round(pupil_obb_data[\"center_y\"])))\n",
        "        current_mrmpd_data[\"pupil_center_px\"] = pupil_center\n",
        "        print(f\"  瞳孔中心 (OBBより): {pupil_center}\")\n",
        "\n",
        "        iris_obb_data = get_obb_data_by_class_name(obb_result_obj, OBB_CLASS_IRIS, obb_model.names, img_idx_for_debug=debug_img_id_str)\n",
        "        if not iris_obb_data: raise ValueError(f\"'{OBB_CLASS_IRIS}' OBBが見つかりません。\")\n",
        "        major_axis_pixels = max(iris_obb_data[\"width\"], iris_obb_data[\"height\"])\n",
        "        if not major_axis_pixels > 0: raise ValueError(f\"'{OBB_CLASS_IRIS}' OBBの長径が0です。\")\n",
        "        pixels_per_mm = major_axis_pixels / CORNEAL_DIAMETER_MM\n",
        "        current_mrmpd_data[\"pixels_per_mm\"] = pixels_per_mm\n",
        "        current_mrmpd_data[\"iris_major_axis_px\"] = major_axis_pixels\n",
        "        print(f\"  虹彩の長径 (OBBより): {major_axis_pixels:.2f}px。計算されたスケール: {pixels_per_mm:.2f} px/mm\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"OBBモデル処理中にエラー: {e}\"\n",
        "        print(f\"エラー (画像 {debug_img_id_str}): {error_msg}\")\n",
        "        current_mrmpd_data.update({\"error\": error_msg, \"status\": \"OBBエラー\"})\n",
        "        all_mrmpd_results_data.append(current_mrmpd_data)\n",
        "        continue\n",
        "\n",
        "    # 2. セグメンテーションモデルによる 'Eyelid' マスク取得 (変更なし)\n",
        "    eyelid_mask_area = None\n",
        "    try:\n",
        "        seg_results_list = seg_model(pil_image_clean_crop, verbose=False, conf=0.25, retina_masks=True)\n",
        "        if not seg_results_list: raise ValueError(\"セグメンテーションモデルが結果を返しませんでした。\")\n",
        "        seg_result_obj = seg_results_list[0]\n",
        "        eyelid_mask_area = get_mask_by_class_name(seg_result_obj, SEG_CLASS_EYELID, seg_model.names, (img_h, img_w), img_idx_for_debug=debug_img_id_str)\n",
        "        if eyelid_mask_area is None: raise ValueError(f\"'{SEG_CLASS_EYELID}' マスクが見つかりません。\")\n",
        "        print(f\"  '{SEG_CLASS_EYELID}' マスク取得成功。\")\n",
        "    except Exception as e:\n",
        "        error_msg = f\"セグメンテーションモデル処理中にエラー: {e}\"\n",
        "        print(f\"エラー (画像 {debug_img_id_str}): {error_msg}\")\n",
        "        current_mrmpd_data.update({\"error\": error_msg, \"status\": \"Segエラー\"})\n",
        "        all_mrmpd_results_data.append(current_mrmpd_data)\n",
        "        continue\n",
        "\n",
        "    # 3. 放射線と 'Eyelid' 領域マスクとの交点計算 (★ロジック変更箇所★)\n",
        "    mpld_mm_values = {}\n",
        "    intersection_points_coords = {}\n",
        "    max_search_radius = int(math.sqrt(img_w**2 + img_h**2))\n",
        "    pupil_cx, pupil_cy = pupil_center\n",
        "\n",
        "    # 瞳孔中心がマスク内にあるか最初に確認\n",
        "    if not (0 <= pupil_cx < img_w and 0 <= pupil_cy < img_h and eyelid_mask_area[pupil_cy, pupil_cx] > 0):\n",
        "        print(f\"警告 (画像 {debug_img_id_str}): 瞳孔中心 ({pupil_center}) がEyelidマスクの範囲外、またはマスク値が0です。MPLD計算は不正確になる可能性があります。\")\n",
        "        # この場合でも処理を続行するが、結果は期待できないかもしれない\n",
        "\n",
        "    print(f\"  MPLD計算中 (瞳孔中心: {pupil_center}):\")\n",
        "    for angle_deg in RADIAL_ANGLES_DEG:\n",
        "        angle_rad = math.radians(angle_deg)\n",
        "\n",
        "        last_known_mask_point = None # マスク領域内だった最後の点を記録\n",
        "        current_distance_at_last_mask_point = np.nan\n",
        "\n",
        "        found_exit_point_for_this_angle = False\n",
        "\n",
        "        for r_step in range(1, max_search_radius + 1): # 瞳孔中心の隣(r=1)から探索開始\n",
        "            x_curr = int(round(pupil_cx + r_step * math.cos(angle_rad)))\n",
        "            y_curr = int(round(pupil_cy - r_step * math.sin(angle_rad)))\n",
        "\n",
        "            if not (0 <= x_curr < img_w and 0 <= y_curr < img_h): # 現在の探索点が画像範囲外\n",
        "                # 画像範囲外に出る直前までが探索範囲。\n",
        "                # last_known_mask_point が None でなければ、それが縁。\n",
        "                if last_known_mask_point is not None:\n",
        "                    mpld_mm_values[angle_deg] = current_distance_at_last_mask_point / pixels_per_mm\n",
        "                    intersection_points_coords[angle_deg] = last_known_mask_point\n",
        "                    found_exit_point_for_this_angle = True\n",
        "                break # この方向の探索は終了\n",
        "\n",
        "            if eyelid_mask_area[y_curr, x_curr] > 0: # 現在点がマスク内\n",
        "                last_known_mask_point = (x_curr, y_curr)\n",
        "                current_distance_at_last_mask_point = r_step # 瞳孔中心からの直線距離(ピクセル)\n",
        "            else: # 現在点がマスク外 (eyelid_mask_area[y_curr, x_curr] == 0)\n",
        "                  # 直前の last_known_mask_point が「出る点」だったことになる\n",
        "                if last_known_mask_point is not None: # 瞳孔中心から少なくとも1ピクセルはマスク内を通った場合\n",
        "                    mpld_mm_values[angle_deg] = current_distance_at_last_mask_point / pixels_per_mm\n",
        "                    intersection_points_coords[angle_deg] = last_known_mask_point\n",
        "                    found_exit_point_for_this_angle = True\n",
        "                # else: 瞳孔中心のすぐ隣(r=1)が既にマスク外。この場合、last_known_mask_pointはNoneのまま。距離は0またはNaN。\n",
        "                #       この場合は、found_exit_point_for_this_angle は False のまま。\n",
        "                break # この方向の探索は終了\n",
        "\n",
        "        if not found_exit_point_for_this_angle:\n",
        "            # ループを完走しても出る点が見つからなかった (画像端までマスク内だったか、そもそもマスクに入らなかった)\n",
        "            if last_known_mask_point is not None: # 画像端までマスク内だった\n",
        "                mpld_mm_values[angle_deg] = current_distance_at_last_mask_point / pixels_per_mm\n",
        "                intersection_points_coords[angle_deg] = last_known_mask_point\n",
        "            else: # 瞳孔中心の隣からずっとマスク外だった場合など\n",
        "                mpld_mm_values[angle_deg] = np.nan\n",
        "                intersection_points_coords[angle_deg] = None\n",
        "\n",
        "    current_mrmpd_data[\"mpld_mm\"] = mpld_mm_values\n",
        "    current_mrmpd_data[\"intersections_px\"] = intersection_points_coords\n",
        "    current_mrmpd_data[\"radial_angles_deg\"] = RADIAL_ANGLES_DEG\n",
        "    print(f\"  MPLD計算完了。\")\n",
        "\n",
        "    # 4. 結果の描画、極座標プロット、非対称比の計算 (変更なし)\n",
        "    img_viz_mrmpd_pil = pil_image_clean_crop.copy().convert(\"RGB\")\n",
        "    draw = ImageDraw.Draw(img_viz_mrmpd_pil)\n",
        "    draw.ellipse((pupil_cx-3, pupil_cy-3, pupil_cx+3, pupil_cy+3), fill=\"red\", outline=\"red\")\n",
        "    for angle_deg, int_pt in intersection_points_coords.items():\n",
        "        if int_pt:\n",
        "            draw.line([(pupil_cx, pupil_cy), int_pt], fill=\"yellow\", width=2)\n",
        "            draw.ellipse((int_pt[0]-3, int_pt[1]-3, int_pt[0]+3, int_pt[1]+3), fill=\"cyan\", outline=\"cyan\")\n",
        "        else:\n",
        "            angle_rad = math.radians(angle_deg)\n",
        "            end_x_no_int = int(round(pupil_cx + 30 * math.cos(angle_rad)))\n",
        "            end_y_no_int = int(round(pupil_cy - 30 * math.sin(angle_rad)))\n",
        "            draw.line([(pupil_cx, pupil_cy), (end_x_no_int, end_y_no_int)], fill=(128,128,128,180), width=1)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img_viz_mrmpd_pil)\n",
        "    plt.title(f\"MRMPD解析結果 - 画像 {debug_img_id_str}\\n瞳孔中心: {pupil_center}, スケール: {pixels_per_mm:.2f} px/mm\")\n",
        "    plt.axis(\"off\")\n",
        "    viz_filename = os.path.join(output_dir, f\"mrmpd_visualization_{debug_img_id_str}.png\")\n",
        "    try: img_viz_mrmpd_pil.save(viz_filename); print(f\"  可視化画像を保存しました: {viz_filename}\")\n",
        "    except Exception as e: print(f\"  可視化画像の保存に失敗しました: {e}\")\n",
        "    plt.show()\n",
        "\n",
        "    valid_plot_angles_rad = []\n",
        "    valid_plot_radii_mm = []\n",
        "    for angle_deg_plot in RADIAL_ANGLES_DEG:\n",
        "        val_mm = mpld_mm_values.get(angle_deg_plot, np.nan)\n",
        "        if not np.isnan(val_mm):\n",
        "            valid_plot_angles_rad.append(math.radians(angle_deg_plot))\n",
        "            valid_plot_radii_mm.append(val_mm)\n",
        "\n",
        "    if valid_plot_radii_mm:\n",
        "        fig_polar, ax_polar = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(7, 7))\n",
        "        ax_polar.plot(valid_plot_angles_rad, valid_plot_radii_mm, marker='o', linestyle='-')\n",
        "        ax_polar.set_theta_zero_location(\"E\"); ax_polar.set_theta_direction(1)\n",
        "        ax_polar.set_xticks(np.deg2rad(RADIAL_ANGLES_DEG))\n",
        "        ax_polar.set_xticklabels([f\"{angle}°\" for angle in RADIAL_ANGLES_DEG])\n",
        "        max_r_val = max(valid_plot_radii_mm) if valid_plot_radii_mm else 5.0\n",
        "        ax_polar.set_rmax(math.ceil(max_r_val * 1.1 if max_r_val > 0 else 5.0))\n",
        "        ax_polar.set_title(f\"MPLDの極座標プロット (mm) - 画像 {debug_img_id_str}\", va='bottom', pad=20)\n",
        "        polar_plot_filename = os.path.join(output_dir, f\"mrmpd_polar_plot_{debug_img_id_str}.png\")\n",
        "        try: fig_polar.savefig(polar_plot_filename); print(f\"  極座標プロットを保存しました: {polar_plot_filename}\")\n",
        "        except Exception as e: print(f\"  極座標プロットの保存に失敗しました: {e}\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"  画像 {debug_img_id_str} の極座標プロットを作成するための有効なMPLDデータがありません。\")\n",
        "\n",
        "    asymmetry_ratios_calculated = {}\n",
        "    ratio_angle_pairs = [(105, 75), (120, 60), (135, 45), (150, 30), (165, 15), (180, 0)]\n",
        "    print(\"  左右非対称比 (側頭側 / 鼻側):\")\n",
        "    for t_angle, n_angle in ratio_angle_pairs:\n",
        "        temporal_val = mpld_mm_values.get(t_angle, np.nan)\n",
        "        nasal_val = mpld_mm_values.get(n_angle, np.nan)\n",
        "        ratio_label = f\"{t_angle}°/{n_angle}°\"\n",
        "        if not np.isnan(temporal_val) and not np.isnan(nasal_val) and nasal_val != 0:\n",
        "            ratio = temporal_val / nasal_val\n",
        "            asymmetry_ratios_calculated[ratio_label] = ratio\n",
        "            print(f\"    {ratio_label}: {ratio:.2f}\")\n",
        "        else:\n",
        "            asymmetry_ratios_calculated[ratio_label] = np.nan\n",
        "            print(f\"    {ratio_label}: N/A (データ不足)\")\n",
        "    current_mrmpd_data[\"asymmetry_ratios\"] = asymmetry_ratios_calculated\n",
        "    current_mrmpd_data[\"status\"] = \"処理完了\"\n",
        "    all_mrmpd_results_data.append(current_mrmpd_data)\n",
        "\n",
        "print(\"\\n--- MRMPD解析 全画像処理完了 ---\")"
      ],
      "metadata": {
        "id": "ztq3qjIApHYb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}